{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Language Modeling 101","text":"<p>Welcome to Language Modeling 101 - an educational repository for understanding and implementing fundamental natural language processing (NLP) techniques from scratch.</p> <p>This project aims to provide clear, well-documented implementations of core language processing algorithms to help learners understand the building blocks of modern NLP systems.</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>Modern language models like GPT, BERT, and T5 are built on a foundation of techniques developed over decades of NLP research. This repository explores these fundamental techniques with clean implementations and detailed explanations.</p>"},{"location":"index.html#topics-covered","title":"Topics Covered","text":""},{"location":"index.html#tokenization","title":"Tokenization","text":"<p>Tokenization is the first step in any NLP pipeline - breaking text into meaningful units for processing:</p> <ul> <li>Byte Pair Encoding (BPE): A subword tokenization algorithm that learns to represent common character sequences as single tokens.</li> </ul>"},{"location":"index.html#project-structure","title":"Project Structure","text":"<ul> <li>Each topic has its own directory with:</li> <li>Implementation files (<code>.py</code>)</li> <li>Detailed README explaining the concept</li> <li>Examples showing usage</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Browse the topics above to learn about specific algorithms, or clone the repository to experiment with the implementations:</p> <pre><code>git clone https://github.com/saqlain2204/Language-Modeling-101.git\ncd Language-Modeling-101\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>Contributions are welcome! If you'd like to add a new algorithm implementation, improve documentation, or fix issues, please feel free to submit a pull request.</p> <p>Created by Saqlain</p>"},{"location":"Tokenization/BPE/index.html","title":"Byte Pair Encoding (BPE)","text":"<p>Basic Idea: Train the tokenizer on raw text to automatically determine the vocabulary.</p> <p>Intuition: Common sequences of characters are represented by a single token, rare sequences are represented by many tokens.</p> <p>Algorithm Sketch: 1. Start with each character as a token (plus an end-of-word marker). 2. Count all adjacent token pairs in the corpus. 3. Merge the most frequent pair into a new token. 4. Repeat for a fixed number of merges.</p>"},{"location":"Tokenization/BPE/index.html#how-it-works","title":"How It Works","text":"<p>BPE is an algorithm originally used for data compression. In NLP, it's used to build subword vocabularies:</p> <ol> <li>Initialization: Start with all characters in your corpus as separate tokens</li> <li>Iterative Merging: Repeatedly merge the most frequent adjacent pair of tokens</li> <li>Stopping Criterion: Stop after a certain number of merges (vocabulary size)</li> </ol>"},{"location":"Tokenization/BPE/index.html#step-by-step-bpe-process","title":"Step-by-Step BPE Process","text":"<p>Let's walk through a simplified example of how BPE works with these words: <pre><code>low lower lowest\nnewer wider\n</code></pre></p>"},{"location":"Tokenization/BPE/index.html#initial-vocabulary","title":"Initial Vocabulary","text":"<p>Each word is split into characters with an end-of-word marker: - <code>l, o, w, &lt;/w&gt;</code> (low) - <code>l, o, w, e, r, &lt;/w&gt;</code> (lower) - <code>l, o, w, e, s, t, &lt;/w&gt;</code> (lowest) - <code>n, e, w, e, r, &lt;/w&gt;</code> (newer) - <code>w, i, d, e, r, &lt;/w&gt;</code> (wider)</p>"},{"location":"Tokenization/BPE/index.html#first-few-merge-operations","title":"First Few Merge Operations","text":"<ol> <li>Find most frequent pair: <code>e, r</code> (appears 3 times)</li> <li>Merge: <code>e + r \u2192 er</code></li> <li> <p>Vocabulary becomes: <code>l, o, w, &lt;/w&gt;</code>, <code>l, o, w, er, &lt;/w&gt;</code>, <code>l, o, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>l, o</code> (appears 3 times)</p> </li> <li>Merge: <code>l + o \u2192 lo</code></li> <li> <p>Vocabulary becomes: <code>lo, w, &lt;/w&gt;</code>, <code>lo, w, er, &lt;/w&gt;</code>, <code>lo, w, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></p> </li> <li> <p>Find most frequent pair: <code>lo, w</code> (appears 3 times)</p> </li> <li>Merge: <code>lo + w \u2192 low</code></li> <li>Vocabulary becomes: <code>low, &lt;/w&gt;</code>, <code>low, er, &lt;/w&gt;</code>, <code>low, e, s, t, &lt;/w&gt;</code>, <code>n, e, w, er, &lt;/w&gt;</code>, <code>w, i, d, er, &lt;/w&gt;</code></li> </ol> <p>...and so on.</p>"},{"location":"Tokenization/BPE/index.html#visual-progression-for-lowest","title":"Visual Progression for \"lowest\"","text":"<pre><code>Initial:  l   o   w   e   s   t   &lt;/w&gt;\nMerge 1:  l   o   w   e   s   t   &lt;/w&gt;  (er doesn't appear)\nMerge 2:  lo  w   e   s   t   &lt;/w&gt;      (lo created)\nMerge 3:  low e   s   t   &lt;/w&gt;          (low created)\n...\nFinal:    low est &lt;/w&gt;                   (after additional merges)\n</code></pre> <p>This demonstrates how the algorithm gradually builds larger subword units based on frequency.</p>"},{"location":"Tokenization/BPE/index.html#implementation","title":"Implementation","text":"<p>The Python implementation includes several key functions that work together to perform Byte Pair Encoding:</p>"},{"location":"Tokenization/BPE/index.html#1-statistics-collection-function","title":"1. Statistics Collection Function","text":"<pre><code>def get_stats(vocab):\n    pairs = Counter()\n    for word, freq in vocab.items():\n        symbols = word\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i + 1])] += freq\n    return pairs\n</code></pre> <p>This function counts the frequency of adjacent token pairs in the vocabulary. For each word in our vocabulary: - It iterates through all adjacent pairs of symbols - It increments a counter for each pair, weighted by how frequently the word appears - The result is a Counter object mapping each pair to its frequency across the corpus</p>"},{"location":"Tokenization/BPE/index.html#2-vocabulary-merging-function","title":"2. Vocabulary Merging Function","text":"<pre><code>def merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = ''.join(pair)\n    for word in vocab:\n        new_word = []\n        i = 0\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = vocab[word]\n    return new_vocab\n</code></pre> <p>This function applies a specific merge operation to the entire vocabulary: - It creates a new bigram by joining the pair of tokens - For each word in the vocabulary, it replaces all occurrences of the specific pair with the new merged token - It preserves word frequencies in the new vocabulary - It returns a new vocabulary with the merge applied throughout</p>"},{"location":"Tokenization/BPE/index.html#3-bpe-training-function","title":"3. BPE Training Function","text":"<pre><code>def bpe(corpus, num_merges):\n    vocab = Counter()\n    for line in corpus:\n        for word in line.split():\n            vocab[tuple(word) + ('&lt;/w&gt;',)] += 1\n\n    merges = []\n    for _ in range(num_merges):\n        pairs = get_stats(vocab)\n        if not pairs:\n            break\n        best = max(pairs, key=pairs.get)\n        vocab = merge_vocab(best, vocab)\n        merges.append(best)\n    return merges, vocab\n</code></pre> <p>The main BPE algorithm function that: - Initializes a vocabulary where each word is split into individual characters plus an end-of-word marker <code>&lt;/w&gt;</code> - Repeatedly:   - Counts pair frequencies with <code>get_stats()</code>   - Finds the most frequent pair   - Applies the merge operation with <code>merge_vocab()</code>   - Records the merge for later use in encoding - Returns both the sequence of merges and the final vocabulary</p>"},{"location":"Tokenization/BPE/index.html#4-encoding-function","title":"4. Encoding Function","text":"<pre><code>def encode(word, merges):\n    word = tuple(word) + ('&lt;/w&gt;',)\n    for pair in merges:\n        i = 0\n        new_word = []\n        while i &lt; len(word):\n            if i &lt; len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(''.join(pair))\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = tuple(new_word)\n    # Handle the end-of-word marker\n    if word[-1] == '&lt;/w&gt;':\n        return word[:-1]\n    else:\n        last = word[-1].replace('&lt;/w&gt;', '')\n        return word[:-1] + (last,) if last else word[:-1]\n</code></pre> <p>This function tokenizes new words using the learned BPE merges: - It starts with the word split into individual characters plus the end marker - It applies each merge operation in the same order they were learned during training - It handles the end-of-word marker properly when returning the final tokenized word</p>"},{"location":"Tokenization/BPE/index.html#usage-example","title":"Usage Example","text":"<p>Here's a complete example showing how to use the BPE implementation:</p> <pre><code>from bpe import bpe, encode\n\n# Define a small corpus for training\ncorpus = [\n    \"low lower lowest\",\n    \"newer wider\",\n]\n\n# Train the BPE model with 20 merge operations\nmerges, vocab = bpe(corpus, num_merges=20)\n\n# Print the sequence of merges learned\nprint(\"Learned merges:\")\nfor i, merge in enumerate(merges):\n    print(f\"Merge {i+1}: {merge[0]} + {merge[1]} \u2192 {''.join(merge)}\")\n\n# Print the final vocabulary\nprint(\"\\nFinal vocabulary:\")\nfor word, freq in vocab.items():\n    print(f\"{word}: {freq}\")\n\n# Encode a new word using the learned merges\nencoded = encode(\"lowest\", merges)\nprint(\"\\nEncoded 'lowest':\", encoded)\n</code></pre> <p>This example demonstrates: 1. Training: Creating a BPE model from a small corpus 2. Inspection: Viewing the learned merges and resulting vocabulary 3. Application: Using the model to encode a new word</p> <p>When you run this code, you'll see how the word \"lowest\" gets tokenized according to the subword units learned during training.</p>"},{"location":"Tokenization/BPE/index.html#advantages-of-bpe","title":"Advantages of BPE","text":"<ul> <li>Handles out-of-vocabulary words gracefully</li> <li>Balances word-level and character-level representations</li> <li>Efficient for morphologically rich languages</li> <li>Widely used in modern NLP systems (GPT, BERT, etc.)</li> </ul> <p>Created by Saqlain.</p>"}]}