# Language Modeling

Welcome to the Language Modeling module of NLP-101. This section covers fundamental concepts and implementations in language modeling.

## Topics Covered

| Topic | Subtopic | Description | Resources |
|-------|----------|-------------|-----------|
| Tokenization | Byte Pair Encoding (BPE) | Subword tokenization algorithm that learns to represent common character sequences as single tokens | [BPE Guide](Tokenization/BPE/README.md), [Python Implementation](Tokenization/BPE/bpe.py) |

## What is Language Modeling?

Language modeling is a fundamental task in Natural Language Processing that involves predicting the probability of a sequence of words. It forms the backbone of many modern NLP applications, from machine translation to text generation.

## Learning Objectives

| Objective | Description |
|-----------|-------------|
| Basic Concepts | Understand fundamental concepts of language modeling and probability distributions over text |
| Tokenization Techniques | Learn different tokenization approaches including character, word, and subword tokenization |
| Implementation | Implement and experiment with various language modeling approaches from scratch |