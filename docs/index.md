# Language Modeling 101

Welcome to Language Modeling 101 - an educational repository for understanding and implementing fundamental natural language processing (NLP) techniques from scratch.

This project aims to provide clear, well-documented implementations of core language processing algorithms to help learners understand the building blocks of modern NLP systems.

## Overview

Modern language models like GPT, BERT, and T5 are built on a foundation of techniques developed over decades of NLP research. This repository explores these fundamental techniques with clean implementations and detailed explanations.

## Topics Covered

### Tokenization

Tokenization is the first step in any NLP pipeline - breaking text into meaningful units for processing:

- [**Byte Pair Encoding (BPE)**](./Tokenization/BPE/README.md): A subword tokenization algorithm that learns to represent common character sequences as single tokens.

## Project Structure

- Each topic has its own directory with:
  - Implementation files (`.py`)
  - Detailed README explaining the concept
  - Examples showing usage

## Getting Started

Browse the topics above to learn about specific algorithms, or clone the repository to experiment with the implementations:

```bash
git clone https://github.com/saqlain2204/Language-Modeling-101.git
cd Language-Modeling-101
```

## Contributing

Contributions are welcome! If you'd like to add a new algorithm implementation, improve documentation, or fix issues, please feel free to submit a pull request.

---

Created by [Saqlain](https://github.com/saqlain2204)
